{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamgarai98/UCL_MSc_Project/blob/main/Dataset_Creation_Analysis/Dataset_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook used to create dataframes of image dataset and store as csv files\n",
        "\n"
      ],
      "metadata": {
        "id": "c52rkzWgWqbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "nf9PYCkTpiHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "import os\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Set script location to own development space\n",
        "MY_DEVELOPMENT_SPACE = '/content/drive/MyDrive/NHM_Project/'\n",
        "os.chdir(MY_DEVELOPMENT_SPACE)"
      ],
      "metadata": {
        "id": "OIAhIrMaGPa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa842ef5-1815-4f8e-dc32-25fdf52e7fee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import load_img\n",
        "from keras.utils import img_to_array\n",
        "from keras import applications\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "gqW7gQSEU5TW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f8344789"
      },
      "outputs": [],
      "source": [
        "# Image paths\n",
        "IMAGE_FOLDER_PATH = '/content/drive/MyDrive/NHM_Project/ALICE_data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create lateral and dorsal dataframes from image folder\n",
        "\n",
        "Dataframes have the following as columns: Path to image, target and view angle (lateral or dorsal)"
      ],
      "metadata": {
        "id": "ST92uk0tli7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets=list()\n",
        "full_paths=list()\n",
        "angle=list()\n",
        "\n",
        "FOLDER_NAMES = os.listdir(IMAGE_FOLDER_PATH)\n",
        "\n",
        "for folder_name in FOLDER_NAMES:\n",
        "\n",
        "  folder_path = os.path.abspath(IMAGE_FOLDER_PATH+os.sep+folder_name)\n",
        "  FILE_NAMES=os.listdir(folder_path)\n",
        "\n",
        "  for file_name in FILE_NAMES:\n",
        "\n",
        "    full_path = os.path.abspath(folder_path+os.sep+file_name)\n",
        "    full_paths.append(full_path)\n",
        "    targets.append(folder_name)\n",
        "\n",
        "    if (\"lateral\" not in file_name):\n",
        "        angle.append(\"Dorsal\")\n",
        "    else:\n",
        "        angle.append(\"Lateral\")"
      ],
      "metadata": {
        "id": "RlWjfHn-U55q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make whole dataframe\n",
        "dataset=pd.DataFrame()\n",
        "dataset['image_path']=full_paths\n",
        "dataset['target']=targets\n",
        "dataset[\"angle\"]=angle"
      ],
      "metadata": {
        "id": "yjCNiCHgVGYF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataframe into dorsal and lateral views\n",
        "dorsalDF = dataset[dataset['angle'] == \"Dorsal\"]\n",
        "dorsalDF = dorsalDF.reset_index(drop=True)\n",
        "\n",
        "lateralDF = dataset[dataset['angle'] == \"Lateral\"]\n",
        "lateralDF = lateralDF.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "fSyA7dZ2WGkB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Test/Validation split on each dataset\n",
        "60/20/20 training/test/validation split\n",
        "\n",
        "**Note: since every specimen has both a lateral and dorsal view and is in order in the original dataframe, both the lateral and dorsal dataframes will contain the same specimen in either the train, validation or test dataframe.**\n",
        "\n",
        "\n",
        "If a train test split is done on the original data somewhere else, then the random state (42) should be consistent to reproduce the same dataframes"
      ],
      "metadata": {
        "id": "IwJqXJJbmNHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For lateral view\n",
        "# Create a 60/20/20 train/validation/test split\n",
        "\n",
        "trainDF_lat, validDF_lat = train_test_split(lateralDF,\n",
        "                                              test_size=0.2,\n",
        "                                              random_state=42,\n",
        "                                              shuffle=True\n",
        "                                             )\n",
        "\n",
        "trainDF_lat, testDF_lat = train_test_split(trainDF_lat,\n",
        "                                           test_size=0.25,\n",
        "                                           random_state=42,\n",
        "                                           shuffle=True\n",
        "                                          )"
      ],
      "metadata": {
        "id": "VuG9qgv-mozr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For dorsal view\n",
        "# Create a 60/20/20 train/validation/test split\n",
        "\n",
        "trainDF_dors, validDF_dors = train_test_split(dorsalDF,\n",
        "                                              test_size=0.2,\n",
        "                                              random_state=42,\n",
        "                                              shuffle=True\n",
        "                                             )\n",
        "\n",
        "trainDF_dors, testDF_dors = train_test_split(trainDF_dors,\n",
        "                                             test_size=0.25,\n",
        "                                             random_state=42,\n",
        "                                             shuffle=True\n",
        "                                            )"
      ],
      "metadata": {
        "id": "al37EuEfmoX-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save datasets to google drive"
      ],
      "metadata": {
        "id": "CL3Hrht7mJVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, reset index column for each DF\n",
        "trainDF_lat = trainDF_lat.reset_index(drop=True)\n",
        "validDF_lat = validDF_lat.reset_index(drop=True)\n",
        "testDF_lat = testDF_lat.reset_index(drop=True)\n",
        "\n",
        "trainDF_dors = trainDF_dors.reset_index(drop=True)\n",
        "validDF_dors = validDF_dors.reset_index(drop=True)\n",
        "testDF_dors = testDF_dors.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "MLnYs-lky_LE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DF's to csv and save in the dataframes folder in my dev space\n",
        "dataset.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/dataset.csv', encoding = 'utf-8-sig')\n",
        "\n",
        "lateralDF.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/lateralDF.csv', encoding = 'utf-8-sig')\n",
        "dorsalDF.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/dorsalDF.csv', encoding = 'utf-8-sig')\n",
        "\n",
        "trainDF_lat.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/trainDF_lat.csv', encoding = 'utf-8-sig')\n",
        "validDF_lat.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/validDF_lat.csv', encoding = 'utf-8-sig')\n",
        "testDF_lat.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/testDF_lat.csv', encoding = 'utf-8-sig')\n",
        "\n",
        "trainDF_dors.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/trainDF_dors.csv', encoding = 'utf-8-sig')\n",
        "validDF_dors.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/validDF_dors.csv', encoding = 'utf-8-sig')\n",
        "testDF_dors.to_csv(MY_DEVELOPMENT_SPACE + 'dataframes/testDF_dors.csv', encoding = 'utf-8-sig')\n",
        "\n",
        "# To import run like following with argument index_col=0\n",
        "# dorsalDF=pd.read_csv('/content/drive/MyDrive/development/adam/dorsalDF.csv',index_col=0)"
      ],
      "metadata": {
        "id": "Y4YVe_WKx0i4"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}